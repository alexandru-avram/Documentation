{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ecef1e",
   "metadata": {
    "papermill": {
     "duration": 0.004317,
     "end_time": "2022-01-24T15:39:04.023136",
     "exception": false,
     "start_time": "2022-01-24T15:39:04.018819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Web Scraping\n",
    "**Web scraping** or **web harvesting** is either a manual or an automated process through which data is available on websites is extracted. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet or loaded into a database. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. \n",
    "\n",
    "\n",
    "## Prerequisites \n",
    "Python, as a versatile language, is very useful in helping automate this process. But, knowing Python and related packages and syntax is not sufficient. A programmer that uses Python must also have at least a basic understanding of **HTML** and **CSS**.\n",
    "\n",
    "HTML stands for Hypertext Markup Language and every website on the internet uses it to display information. If you right click on a website and select \"View Page Source\" you can see the raw HTML of a web page. This is the information that Python will be looking at to grab information from.\n",
    "\n",
    "CSS stands for Cascading Style Sheets, this is what gives \"style\" to a website, including colors and fonts, and even some animations! CSS uses tags such as id or class to connect an HTML element to a CSS feature, such as a particular color.\n",
    "\n",
    "In some instances, there might be some **JavaScript** that is used to define the interactive elements of a webpage. But, as long as you stick to the HTML code, you just be able to bypass any JS code.\n",
    "\n",
    "## Best Practices\n",
    "There are certain rules to follow in order to be able to web scrape successfully. Most important, is to keep in mind that you should have permission to be able to web scrape.  Check a websites terms and conditions for more info.\n",
    "\n",
    "Keep in mind that if you are sending requests to a website that does allow for automated web scrapping, you might get your IP blocked. Some websites might have software that blocks scraping.\n",
    "\n",
    "Every website is unique, so you are not able to clone your code for other websites.\n",
    "\n",
    "Also, because websites change all the time, you must be sure to design your code so that you are able to both adapt it and keep in in the same mindset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df4926",
   "metadata": {
    "papermill": {
     "duration": 0.00301,
     "end_time": "2022-01-24T15:39:04.029925",
     "exception": false,
     "start_time": "2022-01-24T15:39:04.026915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Web Scraping Libraries\n",
    "\n",
    "There are a number of popular modules which are employed for web scraping with Python, but we are going to just focus on the three most popular.\n",
    "\n",
    "## Requests: HTTP for Humans\n",
    "... or simply known as *requests*.\n",
    "\n",
    "**Requests** is a Python library used for making various types of HTTP requests like GET, POST, etc. Because of its simplicity and ease of use, it comes with the motto of HTTP for Humans.\n",
    "\n",
    "[Documentation](https://docs.python-requests.org/en/latest/)\n",
    "\n",
    "## lxml\n",
    "We know the requests library cannot parse the HTML retrieved from a web page. It combines the speed and power of Element trees with the simplicity of Python. It works well when we’re aiming to scrape large datasets. The combination of requests and lxml is very common in web scraping. It also allows you to extract data from HTML using XPath and CSS selectors.\n",
    "\n",
    "[Documentation](https://lxml.de/)\n",
    "\n",
    "## Beautiful Soup\n",
    "BeautifulSoup is perhaps the most widely used Python library for web scraping. It creates a parse tree for parsing HTML and XML documents. One of the primary reasons the Beautiful Soup library is so popular is that it is easier to work with and well suited for beginners. We can also combine Beautiful Soup with other parsers like lxml. But all this ease of use comes with a cost – it is slower than lxml. Even while using lxml as a parser, it is slower than pure lxml.\n",
    "\n",
    "[Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a39f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T15:39:04.040921Z",
     "iopub.status.busy": "2022-01-24T15:39:04.039754Z",
     "iopub.status.idle": "2022-01-24T15:39:04.376574Z",
     "shell.execute_reply": "2022-01-24T15:39:04.375835Z",
     "shell.execute_reply.started": "2022-01-13T14:07:57.87656Z"
    },
    "papermill": {
     "duration": 0.343553,
     "end_time": "2022-01-24T15:39:04.376749",
     "exception": false,
     "start_time": "2022-01-24T15:39:04.033196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "import bs4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.637828,
   "end_time": "2022-01-24T15:39:04.990022",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-24T15:38:53.352194",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
